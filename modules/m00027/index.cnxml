<document xmlns="http://cnx.rice.edu/cnxml">
  <title>How Do We Measure Public Opinion?</title>
  <metadata xmlns:md="http://cnx.rice.edu/mdml">
    <md:title>How Do We Measure Public Opinion?</md:title>
    <md:content-id>m00027</md:content-id>
    <md:uuid>c13d8d5c-5ccf-4df3-9a20-3f8af87f3274</md:uuid>
  </metadata>
  <content>
    <section class="learning-objectives" id="sect-00001">
    <title>Learning Outcomes</title>
    <para id="para-00001">By the end of this section, you will be able to:</para>
    <list id="list-00001">
    <item>Describe different methods of measuring public opinion.</item>
    <item>Explain the shortcomings of these methods.</item>
    </list>
    </section>
    <para id="para-00002">Earlier in this chapter, we discussed how writing to public officials is an important facet of political participation. Before we had a good way to measure public opinion, constituent letters were one of the few ways officials could gauge how the public felt. The advent of <term class="no-emphasis" id="term-00001">public opinion poll</term>s provided a scientific way of identifying and measuring opinions. Social scientist Jean <term class="no-emphasis" id="term-00002">Converse</term>, in her history of the field, writes that surveys can be traced back 2,000 years but were forged in the 20th century as a way to understand mass populations and societies and to gain insight into elites.<cite target-id="rf-00001" id="cite-00001"><note class="reference" display="inline" id="rf-00001">Jean M. Converse, Survey Research in the United States: Roots and Emergence, 1890–1960 (Berkeley: University of California Press, 1987), 1.</note></cite> Over time, polls and surveys have become more precise through careful sampling and improved techniques. A <term id="term-00003">sample</term> is a group selected by a researcher to represent the characteristics of the entire population, and because we can never poll the entire population, getting the right sample is important to the accuracy of any poll. But how can we accurately gauge the opinions of the whole country on a sample of 1,400 or 2,000 people? The way the sample is drawn affects its accuracy. In the most common method, <term id="term-00004">probability sampling</term>, researchers randomly choose samples from the larger population. This method requires that everyone has an equal chance of being part of the sample and that they are randomly selected, which allows researchers to make generalizations about the larger population. If a researcher chooses people at random from a population, it is likely that their views will match the opinions of the larger population as a whole. These types of samples are often generated through <term id="term-00005">random digit dialing</term>, in which respondents are chosen at random by a computerized phone number generator. Researchers then use these randomly generated phone numbers to reach people at home and ask them about their opinions. While random digit dialing has been the go-to for decades, the decrease in landlines, increased adoption of cell phones, and increased time that people are at work have all contributed to the decreased reliance on home-based phone numbers. A <emphasis effect="italics">Los Angeles Times</emphasis> article found that Internet-based surveys and automated interviewing systems (as opposed to live pollsters) were particularly accurate and may reflect a shift in how researchers measure <term class="no-emphasis" id="term-00006">public opinion</term> moving forward.<cite target-id="rf-00002" id="cite-00002"><note class="reference" display="inline" id="rf-00002">David Lauter, “Which Pollsters Did Best: Non-traditional Methods Were Standouts,” Los Angeles Times, November 8, 2012, https://www.latimes.com/politics/la-xpm-2012-nov-08-la-pn-which-pollsters-did-best-election-20121108-story.html.</note></cite></para>
    <para id="para-00003">The difficulty of reaching people for polls is not just an American phenomenon. Researchers in Japan have found steep decreases in responses to nationally conducted surveys, with the steepest declines in metropolitan areas and among younger demographics. Scholars point to increased commute times, longer work hours, and higher mobility among younger Japanese as contributing to this problem.<cite target-id="rf-00003" id="cite-00003"><note class="reference" display="inline" id="rf-00003">Nicolaos E. Synodinos and Shigeru Yamada, “Response Rate Trends in Japanese Surveys,” International Journal of Public Opinion Research 12, no. 1 (March 2000): 48–72.</note></cite> Sampling in countries facing violence or instability can be a serious—and dangerous—problem for pollsters. James <term class="no-emphasis" id="term-00008">Bell</term>, director of international survey research for the Pew Research Center, notes that when Pew conducted polls during civil unrest in Ukraine and Venezuela, polls needed to be conducted face-to-face rather than by phone. In addition, sometimes the data acquired in polls must be processed locally if pollsters cannot immediately evacuate the area.<cite target-id="rf-00004" id="cite-00004"><note class="reference" display="inline" id="rf-00004">James Bell, “Q/A: How Pew Research Tracks Public Opinion in Countries Stricken by Violence and Unrest,” Pew Research Center, May 8, 2014, https://www.pewresearch.org/fact-tank/2014/05/08/qa-how-pew-research-tracks-public-opinion-in-countries-stricken-by-violence-and-unrest/.</note></cite></para>
    <note class="what-do" id="note-00001">
    <title>The Importance of Empirical and Quantitative Skills</title>
    <figure id="fig-00001">
    <media alt="A line graph shows consistent levels of support for political parties in New Zealand between January 1, 2009 and January 1, 2012. The National Party received the most support, with around 50%, the Labour Party received the second most support, with around 30%, and the Green Party received the least support, with around 10%."><image mime-type="image/jpeg" src="../../media/POL_Figure_05_05_014.png"/></media>
    <caption>This graph shows support for political parties in New Zealand between 2009 and 2012, according to various political polls. (credit: “File:NZ opinion polls 2009-2011 -parties.png” Mark Payne, Denmark/Wikimedia Commons, CC BY 3.0)</caption>
    </figure>
    <para id="para-00004">Everyone loves a good public opinion poll. As you’ll see in other chapters, data from polls are utilized throughout society, from the media to candidates running for office, and even to decide what gets included in legislation. However, being able to properly understand what the data are telling us is a skill that is developed and can be utilized in a wide range of areas. If you can look at a set of numerical data or observable facts and reach an informed conclusion about what is happening—for example, about whether a group of voters prefers a certain candidate or if a group of residents wants a park built in their town—this is really no different from determining if a group of consumers prefers brand X bread or brand Y bread. In the modern digital era, we have a wealth of information at our fingertips. Being able to properly understand and interpret that information is a skill that is becoming fundamental in today’s workforce.</para>
    </note>
    <para id="para-00005">There are also “nontraditional” sampling methods, which may be less scientific but offer certain benefits. One nontraditional method is a <term id="term-00009">convenience sample</term>, which, as the name suggests, is a sample based on convenience rather than probability. If you do not have the funds to create a poll based on a <term class="no-emphasis" id="term-00010">probability sample</term> and <term class="no-emphasis" id="term-00011">random digit dialing</term>, you might instead ask your classmates or your coworkers to respond to your survey with their opinions on the last election. While this method is both convenient and easy, we cannot extrapolate much from the information beyond the sample from which it is drawn. Another type of polling method is called <term id="term-00012">cluster sampling</term>, in which researchers divide the overall population into clusters, based on characteristics such as shared cities or schools, then randomly select people from within those clusters to poll. This type of sampling is cheaper than probability sampling, but the results are also not quite as representative because they are not randomly drawn.</para>
    <para id="para-00006">How reliable are polls? One of the most basic issues with a poll is a sample that is too small, which leads to <term id="term-00013">sampling errors</term>. Generally speaking, the larger the sample, the less chance of error. A typical sample of 1,500 people will have a sampling error of approximately 2.6 percent, which is generally considered an “acceptable” margin of error in public opinion polling. This means that out of 1,500 respondents, if 60 percent say that the LA Lakers is their favorite NBA team, due to sampling error, the true figure could be anywhere between 57.4 percent and 62.6 percent who prefer the Lakers. The smaller the sample, the larger the error.</para>
    <para id="para-00007">The methods by which respondents are contacted can also affect a poll’s accuracy. According to the Centers for Disease Control and Prevention (CDC), in 2020, 83 percent of Americans aged 30–34, 74.5 percent of those aged 35–44, and almost 60 percent of those aged 45–64 used cell phones exclusively.<cite target-id="rf-00005" id="cite-00005"><note class="reference" display="inline" id="rf-00005">Stephen J. Blumberg and Julian V. Luke, Wireless Substitution: Early Release of Estimates from the National Health Interview Survey, January–June 2020 (Washington, DC: US Department of Health and Human Services, February 2021), https://www.cdc.gov/nchs/data/nhis/earlyrelease/wireless202102-508.pdf.</note></cite> This trend away from landlines can contribute to <term id="term-00014">selection bias</term>, whereby the sample drawn is not representative of the population being studied. In this case, any sample drawn from people using landlines would probably skew heavily toward individuals who are much older and those who are likely to be at home more often.</para>
    <para id="para-00008">The design of the survey itself can limit a poll’s accuracy. Question wording, interviewer bias, and response bias can all lead to <term id="term-00015">measurement error</term>, or limitations in response validity due to survey design problems. Questions should be worded in a straightforward manner in order to solicit a truthful response. Studies have shown that alterations in question wording, also known as <term id="term-00016">question wording effects</term>, change how people respond to polls and surveys. For example, University of Chicago Professor Kenneth <term class="no-emphasis" id="term-00017">Rasinki</term> found that even the slightest changes in wording altered people’s support for government spending,<cite target-id="rf-00006" id="cite-00006"><note class="reference" display="inline" id="rf-00006">Kenneth A. Rasinski, “The Effect of Question Wording on Public Support for Government Spending,” Public Opinion Quarterly 53, no. 3 (Autumn 1989): 388–394, https://www.jstor.org/stable/2749127.</note></cite> while Cornell University Professor Jonathon <term class="no-emphasis" id="term-00018">Schuldt</term>, Indiana University Professor Sara <term class="no-emphasis" id="term-00019">Konrath</term>, and University of Southern California Professor Norbert <term class="no-emphasis" id="term-00020">Schwarz</term> found that responses changed depending on whether they used the term “<term class="no-emphasis" id="term-00021">global warming</term>” or the term “<term class="no-emphasis" id="term-00022">climate change</term>.”<cite target-id="rf-00007" id="cite-00007"><note class="reference" display="inline" id="rf-00007">Jonathon P. Schuldt, Sara H. Konrath, and Norbert Schwarz, “‘Global Warming’ or ‘Climate Change’? Whether the Planet Is Warming Depends on Question Wording,” Public Opinion Quarterly 75, no. 1 (Spring 2011): 115–124, https://doi.org/10.1093/poq/nfq073.</note></cite> Bias that stems from the identity of the individual conducting the interview known as <term id="term-00007">interviewer bias</term>, can also change people’s opinions. For example, Princeton University Professor Daniel <term class="no-emphasis" id="term-00024">Katz</term> found that the social class of the interviewer had an effect on survey response,<cite target-id="rf-00008" id="cite-00008"><note class="reference" display="inline" id="rf-00008">Daniel Katz, “Do Interviewers Bias Poll Results?,” Public Opinion Quarterly 6, no. 2 (Summer 1942): 248–268, https://www.jstor.org/stable/2745023.</note></cite> while a study of breast cancer patients found that response rates to surveys were higher when the race of the person administering the survey was the same as that of the respondent.<cite target-id="rf-00009" id="cite-00009"><note class="reference" display="inline" id="rf-00009">Patricia G. Moorman et al., “Participation Rates in a Case-Control Study: The Impact of Age, Race, and Race of Interviewer,” Annals of Epidemiology 9, no. 3 (April 1999): 188–195.</note></cite> Similar effects have been found when interviewers are of different genders.<cite target-id="rf-00010" id="cite-00010"><note class="reference" display="inline" id="rf-00010">Leonie Huddy et al., “The Effect of Interviewer Gender on the Survey Response,” Political Behavior 19, no. 3 (September 1997): 197–220, https://www.jstor.org/stable/586516.</note></cite> In other words, people sometimes respond differently based on the gender of the person conducting the survey.</para>
    <para id="para-00009">Inaccuracies can also arise from <term id="term-00025">response bias</term>, when respondents inaccurately report their true opinions for one reason or another. One famous example of response bias is called the “<term class="no-emphasis" id="term-00026">Bradley effect</term>.” This theory refers to a phenomenon observed in the 1982 California gubernatorial race between Tom <term class="no-emphasis" id="term-00027">Bradley</term>, a Black man, and George <term class="no-emphasis" id="term-00028">Deukmejian</term>, a White man of Armenian descent. In polls leading up to this race, Bradley was shown to be in the lead, but he ultimately lost by a narrow margin. The theory behind the Bradley effect is that White voters are unlikely to admit to bias against minority candidates, and as such, polls may overestimate support for a minority candidate. Also known as <term id="term-00029">social desirability bias</term>, this type of response bias occurs when respondents give the answer they think they <emphasis effect="italics">should</emphasis> give, and not what they really feel.</para>
    <section class="summary" id="sect-00002">
    <title/>
    <para id="para-00010">Measuring public opinion is both a skill and a science. In order to attempt to accurately capture public sentiment, researchers must draw samples from the broader public, which usually entails selecting a probability sample using random digit dialing. Errors can occur if the sample size is too small or if the survey suffers from measurement error. Examples of such errors include selection bias, question wording effects, interviewer effects, response bias, and social desirability.</para>
    </section>
    <section class="review-questions" id="sect-00003">
    <title/>
    <exercise id="exer-00001"><problem id="prob-00001"><para id="para-00011"><link class="os-embed" url="#exercise/PS_Ch5_Sec5_RQ1"/></para></problem></exercise>
    <exercise id="exer-00002"><problem id="prob-00002"><para id="para-00012"><link class="os-embed" url="#exercise/PS_Ch5_Sec5_RQ2"/></para></problem></exercise>
    <exercise id="exer-00003"><problem id="prob-00003"><para id="para-00013"><link class="os-embed" url="#exercise/PS_Ch5_Sec5_RQ3"/></para></problem></exercise>
    </section>
    </content>
    <glossary>
    <definition id="def-00001"><term>cluster sampling</term> <meaning>when researchers divide the overall population into clusters, based on characteristics such as shared cities or schools, then randomly select people to poll from within those clusters</meaning></definition>
    <definition id="def-00002"><term>convenience sample</term> <meaning>a sample based on convenience rather than probability</meaning></definition>
    <definition id="def-00003"><term>interviewer bias</term> <meaning>when individual characteristics such as the race or gender of the interviewer affect a person’s survey response</meaning></definition>
    <definition id="def-00004"><term>measurement error</term> <meaning>limitations in response validity due to survey design problems</meaning></definition>
    <definition id="def-00005"><term>probability sampling</term> <meaning>when researchers choose samples at random from the larger population</meaning></definition>
    <definition id="def-00006"><term>question wording effects</term> <meaning>when the wording of the questions on a survey affects how individuals respond</meaning></definition>
    <definition id="def-00007"><term>random digit dialing</term> <meaning>a survey method that selects people for involvement by generating telephone numbers at random</meaning></definition>
    <definition id="def-00008"><term>response bias</term> <meaning>when respondents to a survey inaccurately report their true opinions for one reason or another</meaning></definition>
    <definition id="def-00009"><term>sample</term> <meaning>a group selected by researchers to represent the characteristics of the entire population</meaning></definition>
    <definition id="def-00010"><term>sampling errors</term> <meaning>errors that occur in a statistical analysis due to the unrepresentativeness of the sample</meaning></definition>
    <definition id="def-00011"><term>selection bias</term> <meaning>when the method by which a sample is chosen causes the sample to be unrepresentative of the population being studied</meaning></definition>
    <definition id="def-00012"><term>social desirability bias</term> <meaning>when respondents answer survey questions in a manner intended to cause them to be viewed favorably by others</meaning></definition>
    </glossary>
    </document>